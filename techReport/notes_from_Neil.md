1. We should use the term "value difference" and "attention difference" or perhaps, in symbols DELTA V and DELTA A. It is too confusing to use value for both the value and the difference.
   - Have done thisâ€¦ most of it was like this anyways?
2. Dividing by RT for attention difference is strange. No attention to right would be a zero. No attention to left could be 0.8 or 0.7 or 0.9, depending on the fraction of time looking elsewhere on a trial. Strange.
   - CERE: It works out as the difference in proportion of the trial spent looking at option A vs option B (which is a phrasing that makes slightly more sense.)
   - CERE: Also, doing it any other way makes no difference (See Section Looking at different ways of operationalizing attention) as they are all strongly correlated (>98%)
3. Were outlying trials done per task. We seem to have twice as many strength-of-preference trials removed, just because the binary task was faster.
   - This is what we pre-registered. I raised this issue and Tim said it didn't matter. 
4. Figure 2. Legend should be "Value difference"? Swap attention difference and value difference, so that there are four lines for six values on the x-axis. Figure 2 needs some CIs. Also need to have data and model fit on same figure. 
   - Nice way of CI? 
   - What do you mean? is this 2x2 graphs?
5. We say we are only using the first block, but then do an interaction with order on p. 7.
6. The whole point of the experiment was to see whether the DELTA A by DELTA V interaction was significant. We need to have an _estimation_ of the size of the interaction in each task, and an estimation of the _size_ of the difference in the interactions between the tasks. Just showing large p-values is not enough---risks inference from the null (concluding no difference from failing to reject the null).
7. Table 3 is supposed to be about the number of fixations, but the DV seems to be about the duration of fixations (see heading "relativeFixDur"). Table 3 is a duplicate of Table 4.






